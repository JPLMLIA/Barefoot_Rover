# -*- coding: utf-8 -*-
"""
Created on Mon Aug 20 11:31:00 2018

@author: marchett, jackal
"""

import argparse
import glob
import logging
import os
import pickle
import subprocess
import sys
from contextlib import closing

import h5py

mpl_logger = logging.getLogger('matplotlib')
mpl_logger.setLevel(logging.WARNING)
import matplotlib as mp
import numpy as np
from matplotlib import pyplot as plt
from sklearn.ensemble import (GradientBoostingClassifier,
                              GradientBoostingRegressor)
from sklearn.metrics import (accuracy_score, confusion_matrix,
                             mean_squared_error)
from sklearn.model_selection import GroupKFold
from sklearn.preprocessing import StandardScaler
from typing import Dict,Union, Tuple
from bf_config import bf_globals
from bf_logging import bf_log
from bf_plot import plots_pg
from bf_util import h5_util

plt.set_cmap('afmhot')

bf_log.setup_logger()
logger = logging.getLogger(__name__)

def train_model(feature_file: Union[str, Dict[str,np.ndarray]],
                classdir: str, model_type: str, module: str,
                version: str, date: str) \
                    -> Tuple[float,float]:
    """Saves plots, features and model in classdir

    [extended_summary]

    Parameters
    ----------
    feature_file : Union[str, Dict[str,np.ndarray]]
        Dictionary of features or path to h5 file generated by
        compute_features(*_dense.h5)\n
    classdir : str
        Path to store output\n
    model_type : str
        Regressor or classifier\n
    module : str
        Rock, slip, patterns or material\n
    version : str
        Model version\n
    date : str
        UTC date\n

    Returns
    -------
    Tuple[float,float]
        accuracy/root-mean-squared-error, time to compute
    """
    if(model_type != "classifier" and model_type != "regressor"):
        error_msg = "train_model: model_type must be classifier or regressor."
        logger.error(error_msg)
        sys.exit(error_msg)

    plotdir = os.path.join(classdir, 'plots')
    if not os.path.exists(plotdir):
        logger.info(f"Creating plot directory: {plotdir}")
        os.makedirs(plotdir)

    classifier_file = f"{classdir}/{'_'.join(['classifier', module, version, date])}.h5"
    model_file = f"{classdir}/{'_'.join(['trained_GB', module, version, date])}"
    check_file = glob.glob(model_file)
    if len(check_file) > 0:
        os.remove(model_file)

    # data formating and clean up
    if isinstance(feature_file, dict):
        data = feature_file
    else:
        feature_location = f"{classdir}/{feature_file}_dense.h5"
        logger.info(f"Loading features from {feature_location}")
        data = h5_util.load_h5(feature_location)
    logger.info(f"Training matrix X: {data['X'].shape}")

    samples, features = data["X"].shape
    if(samples <= 0 or features <= 0):
        error_msg = (f"Too few samples or features to create classifier. "
                     f"Samples={samples} Features={features}")
        logger.error(error_msg)
        sys.exit(error_msg)

    if module == 'slip':
        logger.debug("Removing slip values less than 0 or higher than 80%")
        mask_edges = (data['y'] >= 0) & (data['y'] <= 1.0)
        for k in list(data):
            if len(np.atleast_1d(data[k])) == samples:
                data[k] = data[k][mask_edges]
        samples, features = data["X"].shape


    #BOOSTED TREE CLASSIFIER ---------
    S = 3
    np.random.seed(5432)

    prob = [None] * S
    rmse = np.zeros((S, ))
    imps = np.zeros((S, data['nF']))
    y_pred = np.zeros((len(data['X']), S))

    for s in range(S):
        gkf = GroupKFold(n_splits = 10)
        if model_type == 'classifier':
            prob[s] = np.zeros((len(data['X']), len(np.unique(data['y']))))

        for train_idx, test_idx in gkf.split(data['X'], data['y'], groups=data['files']):

            X_train = data['X'][train_idx, :]
            y_train = data['y'][train_idx]

            X_test = data['X'][test_idx, :]
            y_test = data['y'][test_idx]

            #-------------
            #gradient boosting
            if model_type == 'regressor':
                gb = GradientBoostingRegressor(n_estimators = 600, max_depth = 4,
                                               subsample = 0.5,
                                               min_samples_leaf = 100,
                                               max_features = int(data['nF'] / 3), loss = 'ls')
                gb.fit(X_train, y_train)
                y_gb = gb.predict(X_test)
                y_pred[test_idx, s] = y_gb
                rmse[s] = np.sqrt(mean_squared_error(y_test, y_gb))
                prob = np.nan

            if model_type == 'classifier':
                y_train = y_train.astype(int)
                y_test = y_test.astype(int)
                gb = GradientBoostingClassifier(n_estimators = 50, max_depth = 4,
                                                subsample = 0.5, min_samples_leaf = 100,
                                                max_features = int(data['nF'] / 3))
                gb.fit(X_train, y_train)
                y_gb = gb.predict(X_test)
                y_pred[test_idx, s] = y_gb
                prob[s][test_idx, :] = gb.predict_proba(X_test)


            imps[s, :] = gb.feature_importances_

            #PICKLE THE TRAINED MODEL
            with open(model_file, 'ab') as model:
                _ = pickle.dump(gb, model)

    data['y_pred'] = y_pred
    data['feature_imps'] = imps
    data['prob'] = prob
    h5_util.save_h5(classifier_file, data)

    if model_type == 'regressor':
        yhat = y_pred.mean(axis = 1)
        rmse = np.sqrt(mean_squared_error(data['y'], yhat))
        logger.info(f"Done, rmse = {rmse}")

        figs, names = plots_pg.regressorPlots(data, y_pred, imps, version, n_top = 5)
        for g in range(len(names)):
            figs[g].savefig(f"{plotdir}/{names[g]}.png", dpi = 100)
            plt.close()

        return rmse, data['time_to_compute']

    if model_type == 'classifier':
        uclass = np.unique(data['y'])
        prob_mean = np.array(prob).mean(axis = 0)
        y_pred = np.argmax(prob_mean, axis = 1)
        #align the class index
        for u in range(len(uclass)):
            mask_class = y_pred == u
            y_pred[mask_class] = uclass[u]

        acc = accuracy_score(data['y'], y_pred)

        logger.info(f"Done, accuracy = {acc}")

        figs, names = plots_pg.classifierPlots(data, y_pred, prob_mean, imps, module)
        for g in range(len(names)):
            figs[g].savefig(f'{plotdir}/{names[g]}.png', dpi=100)
            plt.close()

        return acc, data['time_to_compute']


if __name__ == "__main__":

    parser = argparse.ArgumentParser()

    parser.add_argument('--featurefile',    help="H5 containing features generated with compute_features.py")

    parser.add_argument('--classdir',       help="Path to store outputs")

    parser.add_argument('--model_type',     help="classifier or regressor")

    parser.add_argument('--module',         help="Barefoot classifier type.  Ex: rock, patterns, hydrations")

    parser.add_argument('--version',        help="Version string.  Example: V1")

    parser.add_argument('--date',           help="Date string. Example: 01242019 for January 24th, 2019")

    args = parser.parse_args()

    #TODO Date should probably not be set by user?
    train_model(args.featurefile, args.classdir, args.model_type, args.module, args.version, args.date)
